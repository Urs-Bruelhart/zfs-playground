#!/bin/bash
#
# This script is used to create sample files for testing purposes.
#

# Errors are fatal
set -e

if test ! "$2"
then
	echo "! "
	echo "! Syntax: $0 filename len_in_bytes"
	echo "! "
	exit 1
fi


FILE=$1
SIZE=$2

#
# Minimum size will be 1K, and that also makes a good block size for relatively small files.
#
BLOCK_SIZE=1024

if test ${SIZE} -lt ${BLOCK_SIZE}
then
	echo "! "
	echo "! Requested size of ${SIZE} is less than block size ${BLOCK_SIZE}. Bailing out!"
	echo "! "
	exit 1
fi

#
# Create a string that is 80 characters long
#
STR=""
for I in $(seq 8)
do
	STR="${STR}XXXXXXXXXX"
done


#
# Get the number of blocks to write, and the remainder which would be for the final block.
#
COUNT=$(( ${SIZE} / ${BLOCK_SIZE} ))
REMAINDER=$(( ${SIZE} % ${BLOCK_SIZE} ))


echo "Creating ${FILE} that is ${SIZE} bytes with a block size of ${BLOCK_SIZE}..."
#
# There's a good writeup at https://unix.stackexchange.com/a/121888/419242 about why
# using dd is a really bad idea.  And while I understand the points the person was making, 
# there is one thing they missed: memory usage.  Using something like "head -c" for a 1 MB file 
# will work, but for a 100 MB file that will turn into 100 Megs of RAM being allocated and
# then deallocated for each file being written.  That's something I'd like to avoid.
#
# dd, on the other hand, is very good about RAM usage, especially with smaller block sizes.
#
while true; do echo ${STR}; done | dd of=${FILE} bs=${BLOCK_SIZE} count=${COUNT} iflag=fullblock 2>/dev/null


if test "${REMAINDER}" -ne 0
then
	while true; do echo ${STR}; done | dd bs=${REMAINDER} count=1 iflag=fullblock >> ${FILE} 2>/dev/null
fi


